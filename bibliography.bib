@incollection{LITTMAN1994157,
title = {Markov games as a framework for multi-agent reinforcement learning},
editor = {William W. Cohen and Haym Hirsh},
booktitle = {Machine Learning Proceedings 1994},
publisher = {Morgan Kaufmann},
address = {San Francisco (CA)},
pages = {157-163},
year = {1994},
isbn = {978-1-55860-335-6},
doi = {https://doi.org/10.1016/B978-1-55860-335-6.50027-1},
url = {https://www.sciencedirect.com/science/article/pii/B9781558603356500271},
author = {Michael L. Littman},
abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsis-tic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.}
}

@book{weiss1999multiagent,
  title={Multiagent systems: a modern approach to distributed artificial intelligence},
  author={Weiss, Gerhard},
  year={1999},
  publisher={MIT press}
}

@misc{aguzzi,
title= {Collective Learning},
author = {Gianluca Aguzzi},
year = {2021},
note= {Lesson on Collective Learning},
}

@article{bettini2022vmas,
  title = {VMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learning},
  author = {Bettini, Matteo and Kortvelesy, Ryan and Blumenkamp, Jan and Prorok, Amanda},
  year = {2022},
  journal={The 16th International Symposium on Distributed Autonomous Robotic Systems},
  publisher={Springer}
}

@ARTICLE{4445757,
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 
  title={A Comprehensive Survey of Multiagent Reinforcement Learning}, 
  year={2008},
  volume={38},
  number={2},
  pages={156-172},
  doi={10.1109/TSMCC.2007.913919}}

  @article{DBLP:journals/corr/MnihKSGAWR13,
  author       = {Volodymyr Mnih and
                  Koray Kavukcuoglu and
                  David Silver and
                  Alex Graves and
                  Ioannis Antonoglou and
                  Daan Wierstra and
                  Martin A. Riedmiller},
  title        = {Playing Atari with Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1312.5602},
  year         = {2013},
  url          = {http://arxiv.org/abs/1312.5602},
  eprinttype    = {arXiv},
  eprint       = {1312.5602},
  timestamp    = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{4421430,
  author={Dung, Le Tien and Komeda, Takashi and Takagi, Motoki},
  booktitle={SICE Annual Conference 2007}, 
  title={Reinforcement learning in non-markovian environments using automatic discovery of subgoals}, 
  year={2007},
  volume={},
  number={},
  pages={2601-2605},
  doi={10.1109/SICE.2007.4421430}}

@INPROCEEDINGS{4655239,
  author={Le Tien Dung and Takashi Komeda and Motoki Takagi},
  booktitle={2008 SICE Annual Conference}, 
  title={Efficient experience reuse in non-Markovian environments}, 
  year={2008},
  volume={},
  number={},
  pages={3327-3332},
  doi={10.1109/SICE.2008.4655239}}


@article{Banchi_2018,
	abstract = {Quantum systems interacting with an unknown environment are notoriously difficult to model, especially in presence of non-Markovian and non-perturbative effects. Here we introduce a neural network based approach, which has the mathematical simplicity of the Gorini--Kossakowski--Sudarshan--Lindblad master equation, but is able to model non-Markovian effects in different regimes. This is achieved by using recurrent neural networks (RNNs) for defining Lindblad operators that can keep track of memory effects. Building upon this framework, we also introduce a neural network architecture that is able to reproduce the entire quantum evolution, given an initial state. As an application we study how to train these models for quantum process tomography, showing that RNNs are accurate over different times and regimes.},
	author = {Leonardo Banchi and Edward Grant and Andrea Rocchetto and Simone Severini},
	doi = {10.1088/1367-2630/aaf749},
	journal = {New Journal of Physics},
	month = {dec},
	number = {12},
	pages = {123030},
	publisher = {IOP Publishing},
	title = {Modelling non-markovian quantum processes with recurrent neural networks},
	url = {https://dx.doi.org/10.1088/1367-2630/aaf749},
	volume = {20},
	year = {2018},
	bdsk-url-1 = {https://dx.doi.org/10.1088/1367-2630/aaf749}}

